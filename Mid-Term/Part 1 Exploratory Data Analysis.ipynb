{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of Zillow Data Set\n",
    "\n",
    "This data is from the Kaggle competition to improve Zillow's \"Zestimate\": https://www.kaggle.com/c/zillow-prize-1\n",
    "\n",
    "The data:\n",
    "\n",
    "* properties_2017.csv: a sample of all properties from 2017 listed on Zillow through Sept\n",
    "* properties_2016.csv: a sample of all properties from 2016 listed on Zillow\n",
    "* train_2017.csv: contains dates, propertyids, and logerror for each transaction in 2017 through Sept\n",
    "* train_2016_v2.csv: contains dates, propertyids, and logerror for each transaction in 2016\n",
    "* Not all properties have transactions\n",
    "* logerror=log(Zestimate)âˆ’log(SalePrice)\n",
    "\n",
    "Goal: find a model that reduces the logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import cm\n",
    "from uszipcode import ZipcodeSearchEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df16 = pd.read_csv('properties_2016.csv', low_memory=False)\n",
    "df_transactions16 = pd.read_csv('train_2016_v2.csv', low_memory=False)\n",
    "df_merged16 = pd.merge(df16, df_transactions16, on='parcelid', how='right')\n",
    "df_merged16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df17 = pd.read_csv('properties_2017.csv', low_memory=False)\n",
    "df_transactions17 = pd.read_csv('train_2017.csv', low_memory=False)\n",
    "df_merged17 = pd.merge(df17, df_transactions17, on='parcelid', how='right')\n",
    "df_merged17.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data above, a few columns need to be cleaned up before we can do exploratory data analysis.\n",
    "\n",
    "First, the latitudes and longitudes are missing their decimal points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_merged17['latitude'] = df_merged17['latitude'] / 1000000\n",
    "df_merged17['longitude'] = df_merged17['longitude'] / 1000000\n",
    "df_merged16['latitude'] = df_merged16['latitude'] / 1000000\n",
    "df_merged16['longitude'] = df_merged16['longitude'] / 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, the tax delinquency years are listed as YY, with the first digit missing if it is a 0. Since some of the years could be from the previous century, we need to fix this so that they will sort in the correct order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def convertyears(x):\n",
    "    if x > 9 and x < 20:\n",
    "        t = '20' + str(x)\n",
    "        return float(t)\n",
    "    elif x <= 9:\n",
    "        t = '200' + str(x)\n",
    "        return float(t)\n",
    "    elif x > 20:\n",
    "        t = '19' + str(x)\n",
    "        return float(t)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "\n",
    "\n",
    "df_merged17['taxdelinquencyyear'] = df_merged17['taxdelinquencyyear'].map(lambda a: convertyears(a))\n",
    "df_merged16['taxdelinquencyyear'] = df_merged16['taxdelinquencyyear'].map(lambda a: convertyears(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also do a quick check of data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_merged16.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transaction dates will be more useful in a datetime format. Categorical columns also have the wrong types, but we will deal with those on a case by case basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "format = '%Y-%m-%d'\n",
    "df_merged16['transactiondate'] = df_merged16['transactiondate'].map(lambda a: datetime.datetime.strptime(a, format))\n",
    "df_merged17['transactiondate'] = df_merged17['transactiondate'].map(lambda a: datetime.datetime.strptime(a, format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pool types have been separated into one-hot columns. We are going to combine them, calculating based on poolcnt and hashottuborspa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pooltypes(a, b):\n",
    "    if a and b > 0:\n",
    "        return 2\n",
    "    elif not(a) and b > 0:\n",
    "        return 7\n",
    "    elif a and b == 0:\n",
    "        return 10\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "df_total['poolcnt'].fillna(0, inplace=True)\n",
    "df_total['hashottuborspa'].fillna(False, inplace=True)\n",
    "\n",
    "df_merged16['pooltype'] = df_merged16.apply(lambda x: pooltypes(x['hashottuborspa'], x['poolcnt']), axis=1)\n",
    "df_merged17['pooltype'] = df_merged17.apply(lambda x: pooltypes(x['hashottuborspa'], x['poolcnt']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some analyses we will be looking at the combined data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_merged16['setyear'] = 2016\n",
    "df_merged17['setyear'] = 2017\n",
    "df_total = df_merged16.append(df_merged17, ignore_index=True)\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map The Log Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.jointplot(x=df_total.latitude.values, y=df_total.longitude.values, size=10)\n",
    "plt.ylabel('Longitude', fontsize=12)\n",
    "plt.xlabel('Latitude', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location features are:\n",
    "* latitude\n",
    "* longitude\n",
    "* regionidzip\n",
    "* regionidcity\n",
    "* regionidcounty\n",
    "* regionidneighborhood\n",
    "* fips\n",
    "* censustractandblock\n",
    "* rawcensustractandblock\n",
    "\n",
    "Let's start with FIPS code, a federal code system for counties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_total['fips'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look up FIPS codes: https://www.census.gov/geo/reference/codes/cou.html\n",
    "\n",
    "Counties in this set:\n",
    "* 6037: LA County \n",
    "* 6059: Orange County \n",
    "* 6111: Ventura County\n",
    "\n",
    "These should map 1:1 to regionidcounty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df_total['fips'],df_total['regionidcounty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two values correspond, so in our feature selection we will use FIPS since that has real-world meaning.\n",
    "\n",
    "Next, we'll look at the ZIP code data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_total['regionidzip'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be an invalid US zip code for the max. Examine all impossible US zip codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp = df_total[df_total['regionidzip'] > 100000]\n",
    "temp['regionidzip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the entries have the same invalid zip. Look at the county the zip code is associated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp['fips'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All have the same county. Get all entries in that county:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp2 = df_total[df_total['fips'] == 6037]\n",
    "temp2['regionidzip'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip code is most likely a military zip code. Let's look at some other features of the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp2['regionidzip'].mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a US zip code. In spot checking, some of the zip codes are from CA, some are from OR, and some don't exist. In our data cleaning we will need to replace the zip code column. \n",
    "\n",
    "Look at the other region identifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_total['regionidcity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_total['regionidneighborhood'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nbcorr = df_total[df_total['fips']==6111]\n",
    "\n",
    "search = ZipcodeSearchEngine()\n",
    "zips = pd.DataFrame(columns=['parcelid','zipcode'])\n",
    "\n",
    "for i, row in nbcorr.iterrows():\n",
    "    b = search.by_coordinate(row['latitude'],row['longitude'])\n",
    "    zips.loc[len(zips)] = [row['parcelid'],b[0].Zipcode]\n",
    "df_neighborhoods = pd.merge(nbcorr, zips, on='parcelid', how='left')\n",
    "df_neighborhoods.groupby('regionidneighborhood')['regionidzip'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most neighborhoods are contained within a single zip code, but some cross zipcode boundaries.\n",
    "\n",
    "Finally, lets look at the census tracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_total['rawcensustractandblock'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_total['censustractandblock'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "float(df_total['rawcensustractandblock'].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "float(df_total['censustractandblock'].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "float(df_total['censustractandblock'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of these values is FIPS code + census tract(six digits, in the format XXXX.XX for the raw version) + census block (four digits). The \"raw\" value appears to have three extra digits and censustractandblock rounds off those digits, but it also has incorrect values (eg the max has an incorrect FIPS code). \n",
    "\n",
    "For the US census, a county is divided into tracts, which is divided into blocks (which can be grouped together based on the first digit). A census block is the smallest geographic unit for which census data is collected, and their numbers are out of 9999 per census tract. If we wanted to examine how neighborhood demographics affect the logerror, we would be able to match census data to groups of properties based on these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_total['rawcensustractandblock'].fillna(0, inplace = True)\n",
    "def census(a): \n",
    "    return int(round(a * 1000000))\n",
    "\n",
    "df_total['rawcensustractandblock'] = df_total['rawcensustractandblock'].map(lambda a: census(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categorical=['airconditioningtypeid','architecturalstyletypeid','buildingclasstypeid',\n",
    "             'decktypeid','heatingorsystemtypeid','storytypeid','typeconstructiontypeid',\n",
    "             'pooltype','propertycountylandusecode','propertylandusetypeid','propertyzoningdesc']\n",
    "numerical = ['basementsqft','bathroomcnt','bedroomcnt','buildingqualitytypeid','calculatedbathnbr',\n",
    "             'finishedfloor1squarefeet','calculatedfinishedsquarefeet','finishedsquarefeet12',\n",
    "             'finishedsquarefeet13','finishedsquarefeet15','finishedsquarefeet50','finishedsquarefeet6',\n",
    "             'fireplacecnt','fullbathcnt','garagecarcnt','garagetotalsqft','latitude','longitude',\n",
    "             'lotsizesquarefeet','poolcnt','poolsizesum','pooltypeid10','pooltypeid2','pooltypeid7','roomcnt',\n",
    "             'threequarterbathnbr','unitcnt','yardbuildingsqft17','yardbuildingsqft26','yearbuilt','numberofstories',\n",
    "             'structuretaxvaluedollarcnt','taxvaluedollarcnt','assessmentyear','landtaxvaluedollarcnt','taxamount',\n",
    "             'taxdelinquencyyear','logerror']\n",
    "flags = ['hashottuborspa','fireplaceflag','taxdelinquencyflag',]\n",
    "\n",
    "\n",
    "df_num=df_total[numerical]\n",
    "df_cat=df_total[categorical]\n",
    "df_flag=df_total[flags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Error Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig,ax=plt.subplots(figsize=(7, 7))\n",
    "ax.set(yscale=\"symlog\")\n",
    "g=sns.distplot(df_num['logerror'].values, bins=50, kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Numerical Data Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "df_num.hist(figsize=(20, 20), bins=50, xlabelsize=8, ylabelsize=8);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data Distribution\n",
    "\n",
    "We can plot categorical data together but need to group them based on similar axis sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "for c in categorical:\n",
    "    sns.countplot(x=df_cat[c], data=df_cat, palette=\"Greens_d\")\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_flag.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Error Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of log errors over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "means = df_total.groupby('transactiondate')['logerror'].mean()\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.scatter(df_total['transactiondate'].tolist(), df_total['logerror'], s =10, c = 'blue')\n",
    "plt.scatter(means.index, means, s =10, c = 'red')\n",
    "plt.title('LogError Over Time')\n",
    "plt.xlabel('Transaction Date')\n",
    "plt.ylabel('Logerror')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_total.groupby('setyear')['logerror'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log error distributions are roughly consistent over time, with the annual means within one standard deviation of each other and an expected decrease in quantity during the winter months since there are fewer properties sold at that time of year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(df_total[numerical].corr(), vmax=.8, square=True, cmap=cm.coolwarm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a slight positive correlation between basementsqft and logerror, as well as a slight negative correlation between taxdelinquencyyear and logerror.\n",
    "\n",
    "More importantly, certain features are strongly correlated with eact other, consistent with the fact that they have the same or similar definitions in the data dictionary. In particular:\n",
    "* bathroomcnt, calcualtedbathnbr, fullbathcnt\n",
    "* finishedfloor1squarefeet, finishedsquarefeet50\n",
    "* calculatedfinishedsquarefeet, finishedsquarefeet12, finsihedsquarefeet13, finishedsquarefeet15, finishedsquarefeet6\n",
    "* structuretaxvaluedollarcnt, taxvaluedollarcnt, landvaluedollarcnt, taxamount\n",
    "\n",
    "Certain features are also correlated that logically would reflect the size of the house - including number of bathrooms, number of bedrooms, total square footage, and the structure tax value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percent Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missing_percents16 = (len(df_merged16.index) - df_merged16.count())/len(df_merged16.index)\n",
    "missing_percents17 = (len(df_merged17.index) - df_merged17.count())/len(df_merged17.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missing_percents16.sort_values(inplace=True)\n",
    "temp = pd.DataFrame(missing_percents17, columns=['2017'])\n",
    "missing_combined = pd.DataFrame(missing_percents16, columns=['2016'])\n",
    "missing_combined = missing_combined.join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missing_combined.plot.barh(figsize=(20,40))\n",
    "plt.yticks(size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Features Both Over and Underestimate the Log Error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
